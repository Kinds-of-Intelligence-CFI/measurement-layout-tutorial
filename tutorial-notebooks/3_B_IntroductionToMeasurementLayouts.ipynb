{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0Z/qrT2cLq+2Q6wiDX3iH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/blob/main/tutorial-notebooks/3_B_IntroductionToMeasurementLayouts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to the Measurement Layout Framework\n",
        "\n",
        "**Lead Presenter**: John Burden"
      ],
      "metadata": {
        "id": "V6oN-jGFcZIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will see how to implement the measurement layouts framework in PyMC. In this tutorial, for simplicity, we will use a synthetic dataset, but in later sessions we will work with real AI systems.\n",
        "\n",
        "This synthetic dataset will mimic a navigation task. An agent will be situated in an arena and will need to find and navigate to a reward. In each instance the meta-features that can change will be the distance from the agent's start point and the reward, as well as the size of the reward. We will chiefly be interested in two capabilities that we will call Navigation and Vision.\n",
        "\n",
        "\n",
        "To start with, let's download the dataset that we will be analyzing. We will also set up a few imports and functions that will be useful later on.\n",
        "\n",
        "\n",
        "If you would like to save any changes you make to the colab make sure to save a copy to your Google Drive!\n"
      ],
      "metadata": {
        "id": "Hrrvo7gCnTzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "import numpy as np\n",
        "import arviz as az\n",
        "import random as rm\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import graphviz\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\")"
      ],
      "metadata": {
        "id": "5JkBsmgQpPoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic(x):\n",
        "  return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def scaledBeta(name, a, b, min, max):\n",
        "\n",
        "  beta = pm.Beta(f\"{name}_raw\", a, b)\n",
        "  return pm.Deterministic(name, beta * (max - min) + min)"
      ],
      "metadata": {
        "id": "Mr44fEWbpVqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/visionNavigationMeasurementLayoutData.csv\")"
      ],
      "metadata": {
        "id": "cT8HyrYEtoQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "IGZTBtSPqHWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we have a dataframe of 50000 task instances. For each of these instances we have the distance and size of reward as well as the success of a particular agent we wish to evaluate.\n",
        "Using df.describe() we can see that this agent succeeds in nearly 32% of instances.\n",
        "We can also see that in this dataset the distances range from 10-100 units, and range from 0.1 to 10 size units."
      ],
      "metadata": {
        "id": "V_mtdgDJqLzA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A simple Measurement Layout"
      ],
      "metadata": {
        "id": "An_SdHXnHGue"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a simple measurement layout where the two meta-features, distance and size affect the agent's performance. We will assume that the task requires capabilities in both navigation and vision to succeed at the task. That is, navigation and vision are non-compensatory. An agent cannot reach a far away reward with high vision capability if it does not also have an high navigation capability.\n",
        "\n",
        "\n",
        "Before we start implementing our model in PyMC, we need to determine the meta-features, capabilities, and linking functions as we wish to express them in the Measurement Layouts framework.\n",
        "\n",
        "Clearly there are going to be two meta-features, corresponding to distance and size. And two capabilities, corresponding to navigation and vision.\n",
        "There will be two partial performance nodes, again, for navigation and vision. And finally there will be the observed performance.\n",
        "\n",
        "The most important parts to specify are the linking functions. How does partial performance depend on capabilities and meta-features?\n",
        "\n",
        "In this model we will make the assumption that navigation performance depends simply on how far away the reward is. That is, on the margin of navigation ability and distance:\n",
        "$$navigationP = \\sigma(navigationAbility - instanceDistance)$$\n",
        "The greater navigationAbility is relative to the instanceDistance, the more likely the agent is to succeed on the navigation aspect of the task. When the two are equal, a probability of success of 0.5 is yielded.\n",
        "\n",
        "Vision, on the other hand, is a bit more complicated. Let us assume that the difficulty in identifying the object comes from its apparent size. Rather than depending on just size or distance, this depends on both.\n",
        "\n",
        "\n",
        "We know from physics that a good approximation for the relation looks like:\n",
        "$$ \\text{apparent size} ‚àù \\frac{\\text{size}}{\\text{distance}} $$\n",
        "\n",
        "So as the object gets further away, it linearly looks smaller, and as the size increases it linearly looks bigger.\n",
        "\n",
        "We want some kind of equation mapping to\n",
        "$$  \\sigma  (lr - \\frac{size}{distance}) $$\n",
        "\n",
        "$lr$ here corresponds to some kind of \"latent resolution\" with which the agent can effectively identify objects.\n",
        "\n",
        "However, note that $\\frac{size}{distance}$ will get smaller as the task gets harder. In general we want to be consistent with bigger, positive margins corresponding with more success, so we will change the equation to:\n",
        "$$   \\sigma(acuity - \\frac{distance}{size}) $$\n",
        "\n",
        "Let's reframe $\\frac{1}{lr}$ as $acuity$ to ensure that larger values for abilities correspond to better performance\n",
        "\n",
        "Also note that we will be dealing with a ratio between 0 and $\\infty$, but for small sizes / distances this might get close to zero, and floating point numbers are awkward. We also need the range to be $(-\\infty, \\infty)$ to allow the sigmoid to give a problability in $[0,1]$.\n",
        "\n",
        "So we will reformulate to\n",
        "\\begin{align*}\n",
        "& visualP= \\sigma(log(acuity) - log(\\frac{distance}{size})) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Now having an acuity ability of $x$ allows the agent to identify  objects where  $\\frac{distance}{size} < x$ with a probability of 0.5. Thus having a better acuity is better.\n",
        "\n",
        "Finally, we will assume that these partial performances are non-compensatory. That is, both are required for overall success.\n",
        "This lets us frame final performance as:\n",
        "\n",
        "$$observedPerformance = navigationP \\times visualP $$\n",
        "\n",
        "\n",
        "Now to implement this into PyMC. Try it for yourself using the template below. Part of the code is done for you, and a solution is located in the hidden cell following.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4yexhBSjsPfW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def setupModel(relevantData):\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "\n",
        "  with  m:\n",
        "    instanceMetafeatureDistance = pm.MutableData(\"InstanceDistances\", relevantData[\"Distance\"])\n",
        "    instanceMetafeatureSize = ### ENTER YOUR CODE HERE\n",
        "    navigationAbility = scaledBeta(\"NavigationAbility\", 1,1, 0, 200)\n",
        "    visualAbility = ### ENTER YOUR CODE HERE\n",
        "\n",
        "    navigationP = pm.Deterministic(\"navigationP\", logistic(navigationAbility - instanceMetafeatureDistance))\n",
        "    visualP = #### ENTER YOUR CODE HERE\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", navigationP*visualP)\n",
        "    observed = pm.Bernoulli(\"ObservedPerformance\", finalP, observed =relevantData[\"Success\"])\n"
      ],
      "metadata": {
        "id": "8H13u2RqsPOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Solution\n",
        "def setupModel(relevantData):\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "\n",
        "\n",
        "\n",
        "  with  m:\n",
        "    instanceMetafeatureDistance = pm.MutableData(\"InstanceDistances\", relevantData[\"Distance\"])\n",
        "    instanceMetafeatureSize = pm.MutableData(\"InstanceSizes\", relevantData[\"Size\"])\n",
        "    navigationAbility = scaledBeta(\"NavigationAbility\", 1,1, 0, 200)\n",
        "    visualAbility = scaledBeta(\"VisualAbility\", 1,1, 0, 1000)\n",
        "\n",
        "    navigationP = pm.Deterministic(\"navigationP\", logistic(navigationAbility - instanceMetafeatureDistance))\n",
        "    visualP = pm.Deterministic(\"visualP\", logistic(np.log(visualAbility) - np.log(instanceMetafeatureDistance/instanceMetafeatureSize)))\n",
        "\n",
        "\n",
        "    finalP = pm.Deterministic(\"FinalP\", navigationP*visualP)\n",
        "    observed = pm.Bernoulli(\"ObservedPerformance\", finalP, observed =relevantData[\"Success\"])\n",
        "  return m"
      ],
      "metadata": {
        "id": "pn2zHJgYwYqc",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m = setupModel(df)"
      ],
      "metadata": {
        "id": "MMIycCp4W-rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Recap"
      ],
      "metadata": {
        "id": "RIqyPneUG7E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly recap what this model is doing.\n",
        "\n",
        "First we create a new model, m, using standard PyMC notation.\n",
        "Within that model, we define a MutableData variable to represent the distance metafeatures. For now, don't worry about why it's a MutableData, this will become apparent later on in the tutorial. We then give the PyMC variable a label for PyMC to use to identify our variable \"InstanceDistances\". We also provide the list of Distance instance values from the dataframe.\n",
        "\n",
        "Second, we create a capability for Navigation, which we call navigationAbility. This is a scaled-beta distribution. This has the exact same properties as a beta distribution, except that the range of values is scaled from 0-1 to, in this case, 0-1000. As a prior, we give it the beta parameters (1,1) which corresponds to an uninformative uniform distribution.\n",
        "\n",
        "Finally, we have a Bernoulli distribution selecting success or failure. We use the logistic function applied to the margin:\n",
        "\n",
        "\n",
        "$$ p=  \\sigma (navigationAbility - InstanceDistance) $$\n",
        "This becomes the parameter of the Bernoulli Distribution\n",
        "$$ Success \\sim Ber(p) $$"
      ],
      "metadata": {
        "id": "4ufcOrYvtUyu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "8oitVwefwZyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use PyMC's model_to_graphviz function to visualise what this probabilistic model looks like. In this diagram oval-shaped nodes are probability distributions, whereas rectangles are deterministic calculations. Rounded rectangles are the MutableData class. Grey nodes are \"observed\".\n",
        "The arrows determine a causal structure or the direction of flow of information. Within the measurement layouts framework these arrows designate the presence of a linking function.\n",
        "\n",
        "Note that our scaled-beta \"NavigationAbility\" is deterministic in this graph because of the way that the scaledbeta function is implemented. Where a deterministic function is applied to stretch out the result yielded from a 0-1 bounded \"NavigationAbility_raw\".\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "s3uFMocVyxJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now perform the inference procedure on the model using the data. This follows the standard PyMC approach:"
      ],
      "metadata": {
        "id": "Z6GXp4xLz7MC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with m:\n",
        "  inferenceData = pm.sample(1000, target_accept=0.95, cores=2)"
      ],
      "metadata": {
        "id": "4YoBqVo-z60q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our model m, we let PyMC infer the most likely values for the capabilities that we have based on the meta-features and the agent's performance."
      ],
      "metadata": {
        "id": "owV5JHNp0GNS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can plot the distribution that we PyMC finds for the capability"
      ],
      "metadata": {
        "id": "2D-imX8T1gBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_posterior(inferenceData[\"posterior\"][[\"NavigationAbility\", \"VisualAbility\"]])\n"
      ],
      "metadata": {
        "id": "qYRvkij11fri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, PyMC has inferred that the agent's navigationAbility is centred around 50 and the visualAbility around 20.\n",
        "But how do we know if these inferences are reasonable? In the next section we will explore this.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dPK_DXIpHXQp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Diagnostics\n",
        "PyMC is powerful and intuitive enough that we can straightforwardly design measurement layouts. But how do we know if the data are appropriate for them, or whether there are any issues with how they were fitted? There are several diagnostics we can run on measurement layouts, of which some of the most useful are presented here.\n",
        "\n",
        "First, we can look at the traces for each of the capabilities. Below, we can visualise the traces. For each chain (we have used 2 chains above), there is a posterior distribution (see the left plot). We want them to look relatively similar to each other, as this means that each chain converged to a similar posterior. On the right, we see a time series plot indicating how often each of the values were sampled in the chain. We want there to be relative homogeneity here, suggesting that all values were sampled a similar number of times. Note, depending on the prior, there might be spikes for certain values (if, for instance, the prior is a Cauchy distribution or something similarly heavy-tailed)."
      ],
      "metadata": {
        "id": "dJK7SiQJId2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(inferenceData[\"posterior\"][[\"NavigationAbility\",\"VisualAbility\"]])"
      ],
      "metadata": {
        "id": "RsrLdKvLcbIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A second diagnostic is an energy plot, which also enables us to check whether the MCMC algorithm (usually, as here, NUTS) has explored the full posterior distribution. In the energy plot, we simply want the distribution of marginal energy during sampling, and distribution of energy transitions between steps (see [here](https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/pymc_overview.html#model-checking) and [here](https://arxiv.org/abs/160400695)), to overlap and look similar. Everything looks good for our agents with the simple measurement layout:"
      ],
      "metadata": {
        "id": "BWKh_vUcKrDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_energy(inferenceData)"
      ],
      "metadata": {
        "id": "PyDyLGyfar0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here BFMI stands for Bayesian Fraction of Missing Information and measures the mismatch between the distributions. Values below 0.3 are known to be problematic."
      ],
      "metadata": {
        "id": "jHVFV-zeRag4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the literature on Bayesian statistics, several convergence diagnostics have been proposed. [Vehtari et al. (2021)](https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full) present a comprehensive overview. Two diagnostics that we can use out of the box to gauge the convergence of multiple chains when fitting a measurement layout are $\\hat{R}$\n",
        " and Effective Sample Size (ESS).\n",
        "\n",
        " $\\hat{R}$ is, roughly, the ratio of the variance mixed across all chains compared to the root mean squared variance of the variance in each individual chain. If the chains are not converging, then the between-chain variance should be higher than the within-chain variance, so values higher than 1 indicate lack of convergence. In practice, Vehtari et al. suggest values higher than 1.01 indicate a lack of convergence. Below, we present $\\hat{R}$\n",
        " for both navigation and visual capabilities.\n",
        "\n",
        "ESS is, roughly, \"how many independent draws contain the same information as the dependent sample obtained by the MCMC algorithm. The higher the ESS the better\" [(Vehtari et al., 2021, p. 672)](https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full). We can distinguish between $ESS_{bulk}$\n",
        " and $ESS_{tail}$\n",
        " too, where the latter is the ESS in the tails of the posterior distribution, outside of the credibility interval. This is especially useful if credibility intervals are to be used downstream in inference. Below, note that the sample size for each chain is 2000 (1000 warm up, 1000 sample; unless you have changed it)."
      ],
      "metadata": {
        "id": "XqhEp8FfLAOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.summary(inferenceData[\"posterior\"][[\"NavigationAbility\",\"VisualAbility\"]])"
      ],
      "metadata": {
        "id": "FQM8_sp3cFbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another way that we can test the inferences of the measurement layout is by using it to predict overall performance on an unseen test set. This is similar in nature to the train-test split that is common in ML.\n",
        "\n",
        "We have a test set prepared. Let's download it:"
      ],
      "metadata": {
        "id": "6qlJxfHc4fMT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "DKbc0Bwp4q-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = pd.read_csv(\"https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/visionNavigationMeasurementLayoutTestData.csv\")"
      ],
      "metadata": {
        "id": "8VRxEPpzt1tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will also need to define a function that will do the prediction for us. This will leverage PyMC's sample_posterior_predictive function."
      ],
      "metadata": {
        "id": "yUBWaX1ANySN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lxKNiGrA_-Ig"
      },
      "outputs": [],
      "source": [
        "def predict(m, trace, relevantData):\n",
        "  with m:\n",
        "\n",
        "    predictions = pm.sample_posterior_predictive(trace, var_names=[\"FinalP\"], return_inferencedata=False,predictions=True,extend_inferencedata=False)\n",
        "    predictionChainRuns = predictions[\"FinalP\"][:,:,0:len(relevantData)]\n",
        "    predictionsInstance = np.mean(predictionChainRuns, (0,1))\n",
        "\n",
        "    return predictionsInstance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the predictions we create a new model with the test data in as meta-features. We then pass it to the predict function along with the learnt trace of the inferred data because this is where the distributions for each capability are kept."
      ],
      "metadata": {
        "id": "jhijtxXqUWhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "m_test = setupModel(df_test)\n",
        "\n",
        "predictions = predict(m_test, inferenceData, df_test)"
      ],
      "metadata": {
        "id": "k0aRZiVsALPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have our predictions we want to evaluate how good they were on our test-set. As our model is predicting probabilities of success we can use the [Brier score](https://en.wikipedia.org/wiki/Brier_score) as a metric for understanding how good our model is at predicting performance.\n",
        "\n",
        "The Brier score is defined as follows\n",
        "$$Brier Score = \\frac{1}{N} \\sum_{i=1}^N(f_i - o_i)^2 $$\n",
        "\n",
        "Where:\n",
        "$N$ is the number of items in the test set. $f_i$ is the prediction made by the model for the $i$th element, and $o_i$ is the actual outcome for the $i$th element.\n",
        "\n",
        "This is, at its core, the Mean Squared Error of the prediction model where we limit the outcomes to binary events.\n",
        "\n",
        "\n",
        "To use the Brier score, let's define the function we need:"
      ],
      "metadata": {
        "id": "MppIQix3cvJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def brierDecomp(preds, outs):\n",
        "    brier = 1 / len(preds) * sum((preds - outs) ** 2)\n",
        "    bins = np.linspace(0, 1, 11)\n",
        "    binCenters = (bins[:-1] + bins[1:]) / 2\n",
        "    binPredInds = np.digitize(preds, binCenters)\n",
        "    binnedPreds = bins[binPredInds]\n",
        "\n",
        "    binTrueFreqs = np.zeros(10)\n",
        "    binPredFreqs = np.zeros(10)\n",
        "    binCounts = np.zeros(10)\n",
        "\n",
        "    for i in range(10):\n",
        "        idx = (preds >= bins[i]) & (preds < bins[i + 1])\n",
        "\n",
        "        binTrueFreqs[i] = np.sum(outs[idx]) / np.sum(idx) if np.sum(idx) > 0 else 0\n",
        "\n",
        "        binPredFreqs[i] = np.mean(preds[idx]) if np.sum(idx) > 0 else 0\n",
        "        binCounts[i] = np.sum(idx)\n",
        "\n",
        "    calibration = np.sum(binCounts * (binTrueFreqs - binPredFreqs) ** 2) / np.sum(binCounts) if np.sum(\n",
        "        binCounts) > 0 else 0\n",
        "    refinement = np.sum(binCounts * (binTrueFreqs * (1 - binTrueFreqs))) / np.sum(binCounts) if np.sum(\n",
        "        binCounts) > 0 else 0\n",
        "\n",
        "    return brier, calibration, refinement"
      ],
      "metadata": {
        "id": "I4b6C4xKfPEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a version of the Brier score that also breaks down the score into two components: calibration and refinement. These will become useful later on."
      ],
      "metadata": {
        "id": "KZXBp0KGfOVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "brierScore, calibration, refinement = brierDecomp(predictions, df_test[\"Success\"])"
      ],
      "metadata": {
        "id": "iX8bDn8pfhrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brierScore"
      ],
      "metadata": {
        "id": "rUTTrvdGfpY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we (hopefully!) see that the Brier Score is low. This corresponds to a good prediction.\n",
        "But how good is this compared to what we could achieve with alternative methods?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cdfojo4kfxo_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparison to XGBoost\n",
        "Let's compare the predictive power of the measurement layouts to XGBoost.\n",
        "First we will need to import the relevant libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "EAeclN0alRLc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from xgboost import XGBClassifier"
      ],
      "metadata": {
        "id": "ceSXsZZSlelY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputFeatures = [\"Distance\", \"Size\"]\n",
        "outputFeature = \"Success\"\n",
        "\n",
        "XTrain=df[inputFeatures]\n",
        "yTrain=df[outputFeature]\n",
        "\n",
        "XTest = df_test[inputFeatures]\n",
        "yTest = df_test[outputFeature]"
      ],
      "metadata": {
        "id": "vC9yiQNVll4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We assign the meta-features to input Features and the observed success as an output feature before collecting all of the examples in the training set together as XTrain and YTrain."
      ],
      "metadata": {
        "id": "ydgIhQJ7l7BO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = XGBClassifier(objective='binary:logistic')\n",
        "# Set up KFold cross-validation\n",
        "\n",
        "model.fit(XTrain, yTrain)\n",
        "# Make predictions on the test data\n",
        "yPredictions = model.predict_proba(XTest)[:, 1]  # Get the probabilities for the positive class\n",
        "brierScoreXGBoost, calibrationXGBoost, refinementXGBoost = brierDecomp(yPredictions, yTest)\n"
      ],
      "metadata": {
        "id": "9QqgyTpml2TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brierScoreXGBoost"
      ],
      "metadata": {
        "id": "mSXCzlQbrV5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Brier score achieved for XGBoost should be about the same as the one for the Measurement Layout.\n",
        "\n",
        "This is a positive result! The measurement layouts allow for high predictive power while also providing *explanatory* power. We can extract capabilities to help us understand the direct limitations of AI systems (here we can identify *why* the agent is failing, navigation or vision). But we don't have to sacrifice predictive power to do so.\n",
        "\n",
        "This concludes the first tutorial on using the Measurement Layouts. The next few tutorials will explore more complex tasks with bigger measurement layouts, as well as applying them to completely different domains."
      ],
      "metadata": {
        "id": "BSSeHp9LrY0D"
      }
    }
  ]
}