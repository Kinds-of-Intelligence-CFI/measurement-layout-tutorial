{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOOpOukhObV6mPwxkSahX1q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/blob/main/tutorial-notebooks/4_BuildingComplexMeasurementLayouts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Complex Measurement Layouts for Cognitive Benchmarks\n",
        "\n",
        "**Lead Presenter**: Kozzy Voudouris\n",
        "\n",
        "In this tutorial, we attempt to build a more complex measurement layout for some real agents. We incrementally build a measurement layout for studying the cognitive capability of object permanence in a complex three-dimensional environment. Finally, We evaluate some agents on a suite of tests."
      ],
      "metadata": {
        "id": "EOQkgyhAQMZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preamble\n",
        "\n",
        "First, let's import the libraries and functions that we will need."
      ],
      "metadata": {
        "id": "uuPrpVEizOiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arviz --quiet\n",
        "!pip install erroranalysis --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip install pymc --quiet"
      ],
      "metadata": {
        "id": "e-_FrQEczOLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import erroranalysis as ea\n",
        "import gc\n",
        "import graphviz\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import random as rm\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "from pymc import model\n",
        "from xgboost import XGBRegressor\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "rng = np.random.default_rng(1997) # set a seed for reproducibility\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\") #Note, colab imports an older version of PyMC by default. This won't cause problems for this tutorial, but may do if you use a different backend (e.g., gpu) and a jax/numpyro sampler. In which case, run `!pip install 'pymc>5.9' --quiet`"
      ],
      "metadata": {
        "id": "nNjpBg-Pzu5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "\n",
        "Now let's import and inspect the data. We have data to explore from RL agents as well as humans."
      ],
      "metadata": {
        "id": "JCmM9ze7vBOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_data_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/4_objectPermanenceDataAgents.csv'\n",
        "opiaagets_dataset = pd.read_csv(agent_data_url)\n",
        "\n",
        "human_data_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/4_objectPermanenceDataHuman.csv'\n",
        "human_dataset = pd.read_csv(human_data_url)"
      ],
      "metadata": {
        "id": "e-D77_5w0O-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the datasets. There are **4202 instances** and **15 metafeatures** for the performances of **4 agents**. There are **1608 instances** and **15 metafeatures** for human performances.\n",
        "\n",
        "The metafeatures are as follows:\n",
        "1. `basicTask` - is the task a basic task? Values (discrete, binary): `0` (No), `1` (Yes).\n",
        "2. `pctbGridTask` - is the task a PCTB Grid task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "3. `pctb3CupTask` - is the task a PCTB Cup task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "3. `cvchickTask` - is the task a CV Chick task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "3. `mainGoalSize` - what size is the goal in the task? Value range: `0.5` - `5.0`.\n",
        "4. `lavaPresence` - does the instance contain lava? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "5. `goalLeft` - is there a goal placed to the left? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "5. `goalRight` - is there a goal placed to the right? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "5. `goalCentre` - is there a goal placed to ahead? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "5. `goalOccluded` - is the goal occluded when the agent starts the episode? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "6. `navigationDistanceGoal` - how far is the goal from the agent? This is calculated is the manhattan distance to the goal, avoiding any obstacles/pits. Value range: `9.0` - `118.0`.\n",
        "6. `euclideanDistanceGoal` - how far is the goal from the agent? This is calculated is the euclidean distance to the goal, not avoiding any obstacles. Value range: `9.0` - `38.40`.\n",
        "7. `navigationTurnsGoal` - how many right-angle turns would the agent take on the trajectory described by `navigationDistanceGoal`. Value range: `0.0` - `13.0`.\n",
        "7. `navigationDistanceChoice` - how far is the choice point from the agent? This is calculated is the manhattan distance to the choice point, avoiding any obstacles/pits. Value range: `1.0` - `53.5`.\n",
        "7. `navigationTurnsChoice` - how many right-angle turns would the agent take on the trajectory described by `navigationDistanceGoal`. Value range: `0.0` - `6.0`.\n",
        "8. `numChoices` - how many choices does the agent have in the task? Value range: `1.0` - `12.0`.\n",
        "\n",
        "The agents performed each of these tasks, and whether they obtained the goal (`1`) or not (`0`) and whether they picked the correct choice (`1`) or not (`0`) was recorded. The agents are as follows:\n",
        "1. `Random_Agent` - An agent which randomly samples one of the 9 actions in the Animal-AI Environment (`no action`, `forwards`, `backwards`, `left rotate`, `right rotate`, `forwards left`, `forwards right`, `backwards left`, `backwards right`). It then takes that action for a number of steps sampled from $U(1, 20)$.\n",
        "2. `Heuristic_Agent` - An agent that navigates towards green goals, following a rigid rule.\n",
        "4. `Dreamer` - A dreamer-v3 agent trained for 10M steps on the set of 2372 basic and practice tasks.\n",
        "6. `PPO` - A PPO agent trained for 10M steps on the set of 2372 basic and practice tasks."
      ],
      "metadata": {
        "id": "EH9a5LS60luD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opiaagets_dataset"
      ],
      "metadata": {
        "id": "sbsoI-vN0j38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "human_dataset"
      ],
      "metadata": {
        "id": "AEMRQ6wHslDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Complex Measurement Layout\n",
        "\n",
        "We will incrementally build a complex measurement layout for studying object permanence. We will evaluate this measurement layout by using the two reference agents, `Random_Agent` and `Heuristic_Agent`, as baselines. Finally, we will apply this measurement layout to the dreamer and PPO agents.\n",
        "\n",
        "## Object Permanence\n",
        "\n",
        "First, let's set up a simple measurement layout for the *Object Permanence* ability.\n",
        "\n",
        "The leaf node of the measurement layout is success in this case, so we want to define it as a Bernoulli, taking a probability of success. Therefore, we'll need a logistic function. Because we are dealing with bounded capabilities, the logistic function means that the probability of success on a task with a minimum demand for an agent with maximum ability is 0.999. Alternatively, for an agent with minimum ability performing on a task with maximum demand, we get a probability of success of 0.001. This is a nice parameterisation of the logistic function for our case of bounded capabilities."
      ],
      "metadata": {
        "id": "jR5RHB45zKpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic999(x, min, max):    # This logistic function ensures that if x is at -(max-min), we get prob 0.001, and if x is at (max-min), we get prob 0.999\n",
        "  x = x - min\n",
        "  max = max - min\n",
        "  x = 6.90675478 * x / max\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "3eNXs1trEy4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try out the logistic here. Let's say you have an ability bounded between 0 and 100. An agent with maximum ability (100) performing on a task with the minimum demand (0) should pass with 0.999 probability. An agent with minimum ability (0) performing on a task with the maximum demand should pass with 0.001 probability. That means the minimum margin is $-(100-0)$ and the maximum margin is $(100-0)$."
      ],
      "metadata": {
        "id": "AsUtTmOXt1b7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xpoints = np.linspace(-100, 100, 1000)\n",
        "ypoints = logistic999(xpoints, 0, 100)\n",
        "plt.plot(xpoints, ypoints)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ACPsTitPt0uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's build an initial measurement layout. Let's define the combined object permanence demand as the distance to the goal (`navigationDistanceGoal`), which serves as a proxy for time under occlusion, multiplied by the number of choice (`numChoices`) multiplied by whether or not a goal is present (`goalOccluded`)."
      ],
      "metadata": {
        "id": "v15dGtcNx9S7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTWHzmLlQHfK"
      },
      "outputs": [],
      "source": [
        "def setupOPModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility =\n",
        "\n",
        "  maxPermAbility =\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility =                     #which prior is appropriate?\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"navigationDistanceGoal\"].values) #this is a proxy for how long the goal is occluded for.\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin =            # define the margin as the ability minus the combined\n",
        "    objPermP =                 # apply the logistic999 transformation with minPermAbility and maxPermAbility\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", objPermP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title A Recommended Solution\n",
        "def setupOPModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - ((goalDist * opTest) * numChoices))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", objPermP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "cellView": "form",
        "id": "k9l6nQEbxx1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPModel(opiaagets_dataset, 'Random_Agent_Success')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "rzjj9VM0GIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this measurement layout on two of our agents: the `Random_Agent` agent and the `Heuristic_Agent`."
      ],
      "metadata": {
        "id": "PJtPtgtqI970"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPModel(opiaagets_dataset, 'Random_Agent_Success')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95, random_seed=rng)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPModel(opiaagets_dataset, 'Heuristic_Agent_Success')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95, random_seed=rng)"
      ],
      "metadata": {
        "id": "8s0nYG1FGvDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the inferred object permanence capability of these agents, by plotting their capabilities as forest plots:"
      ],
      "metadata": {
        "id": "edXKGnXHJPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['objPermAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "52tC1XRwHNKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at the summary statistics from the two measurement layouts:"
      ],
      "metadata": {
        "id": "-pn-N5ZzJ1Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_random = az.summary(data_random['posterior']['objPermAbility'])\n",
        "summary_random"
      ],
      "metadata": {
        "id": "PXUlQ2h6Hd37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_heuristic = az.summary(data_heuristic['posterior']['objPermAbility'])\n",
        "summary_heuristic"
      ],
      "metadata": {
        "id": "a9DVFTaKJ-YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are inferring that the heuristic agent has higher object permanence than the random agent, but both agents are relatively low on object permanence."
      ],
      "metadata": {
        "id": "9Qy1wpYQKDcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Navigation\n",
        "\n",
        "These tasks are fundamentally search tasks. The agent must navigate towards the reward. As such, an agent with object permanence but poor at navigation may fail many of these tasks. Moreover, an agent without object permanence, but that is good at navigating, may accidentally obtain the reward on occasion.\n",
        "\n",
        "The simplest way to frame this is to say that navigation and object permanence are non-compensatory - being good at navigation does not compensate for being bad at object permanence, and vice versa. For the purposes of this tutorial, we can proceed with this formulation, although it may be more accurate to implement an asymmetric compensatory relationship between these too (since, arguably, navigation is more compensatory for object permanence than vice versa).\n",
        "\n",
        "Navigation demands can be implemented in terms of how far away the goal is along with the circuitousness of the route. We can define this as the product of distance and number of turns.\n",
        "\n",
        "Let's extend the measurement layout to include navigation. Let's again define the combined object permanence demand as the distance to the goal (`minDistToGoal`) multiplied by the number of choice (`numChoices`) multiplied by whether or not a goal is present (`goalOccluded`). Let's define navigation as the distance to the goal (`minDistToGoal`) multiplied by the number of turns it takes to get there, avoiding obstacles (`minNumTurnsGoal`)."
      ],
      "metadata": {
        "id": "ILpMtAGVKVS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility =\n",
        "  minNavAbility =\n",
        "\n",
        "  maxPermAbility =\n",
        "  maxNavAbility =\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility =\n",
        "\n",
        "    navAbility =\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin =\n",
        "    objPermP =\n",
        "\n",
        "    navP =\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP =\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "Eai-I-daRPXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title  A Recommended Solution\n",
        "def setupOPNavModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - ((goalDist* opTest) * numChoices ))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999(navAbility - (goalDist * numTurnsGoal), min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zggDBj5z5CYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavModel(opiaagets_dataset, 'Random_Agent_Success')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "BEcPZlisSVdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try this measurement layout on an agent of your choice:"
      ],
      "metadata": {
        "id": "lvqjg_uvsuIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, abilityMin, abilityMax = setupOPNavModel(opiaagets_dataset, 'Random_Agent_Success')\n",
        "with model:\n",
        "  idata = pm.sample(1000, target_accept=0.95, random_seed=rng)"
      ],
      "metadata": {
        "id": "h3JUouDISozT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['objPermAbility']])\n",
        "axes = forest_plot()[0]\n",
        "axes.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "iEnF2gJqSw2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['navAbility']])\n",
        "axes = forest_plot.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "U0dY9fdMTEBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cognitive profile for the both agents are as we expect. The heuristic agent is very good at navigating, it follows a rule to go towards any green goals and away from any red lava. The random agent is less good at navigating.\n",
        "\n",
        "\n",
        "Another feature of the benchmark that ought to affect behaviour is the presence of lava. Let's include that.\n",
        "\n"
      ],
      "metadata": {
        "id": "hViNiItlFC6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have a new metafeature for lava, let's incorporate that into the measurement layout. To do so, we will make use of the Beta prior because the `lavaPresence` is a binary metafeature. We also need to make use of a special margin for these capabilities. Here, if the binary feature is 0, then the margin represents p(success) = 1. If the binary feature is 1, then p(success) = ability."
      ],
      "metadata": {
        "id": "dD-yr0rgFb81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SimplePrMargin(ability, binaryFeature): # must return a value between 0 and 1\n",
        "  return 1-((1-ability)*binaryFeature)  # If binaryFeature is 0 then the margin represents p(success)=1. If binaryFeature = 1 then p(success)=ability"
      ],
      "metadata": {
        "id": "9Rxr5lC1UP1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavLavaModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility =\n",
        "  minNavAbility =\n",
        "  minVAcuityAbility =\n",
        "\n",
        "  maxPermAbility =\n",
        "  maxNavAbility =\n",
        "  maxVAcuityAbility =\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility =\n",
        "\n",
        "    navAbility =\n",
        "\n",
        "    lavaAbility =                         # The Beta prior is appropriate for binary variables like this one.\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin =\n",
        "    objPermP =\n",
        "\n",
        "    navP =\n",
        "\n",
        "\n",
        "    ## For this binary feature, we can use the SimplePrMargin function\n",
        "\n",
        "    lavaP =\n",
        "\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP =\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"ObservedPerformance\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "zHN9xzAXTUVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title A Recommended Solution\n",
        "def setupOPNavLavaModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1, 1)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - ((goalDist * opTest) * numChoices ))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999((navAbility - (goalDist * numTurnsGoal)), min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", SimplePrMargin(lavaAbility, lavaPresence))\n",
        "\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP * lavaP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "EDuHAq09FfsF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavLavaModel(opiaagets_dataset, 'Random_Agent_Success')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "gs4shIoRIs01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what it estimates for one of the agents:"
      ],
      "metadata": {
        "id": "8dUu8JXiJfJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, abilityMin, abilityMax = setupOPNavLavaModel(opiaagets_dataset, 'PPO_Success')\n",
        "with model:\n",
        "  idata = pm.sample(1000, target_accept=0.95, random_seed=rng)"
      ],
      "metadata": {
        "id": "O_5SpQL_JR21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['objPermAbility']])\n",
        "axes = forest_plot.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "4hASOEbHJYyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['navAbility']])\n",
        "axes = forest_plot_random.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "CuwfthdPOS60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "These tasks are heavily affected by what the agent can and cannot see. Depending on the input, agents can struggle to observe smaller, more distant goals. So let's introduce a visual acuity ability.\n",
        "\n",
        "## Introducing Visual Acuity\n",
        "\n",
        "As in the previous tutorial, we define visual acuity in terms of the size and distance of the goal (when it is visible) from the agent at the start of the episode. We can use `mainGoalSize` for the size metafeature and `euclideanDistanceGoal` for the distance metafeature (since we don't care if there are obstacles in the way, like we do with navigation)."
      ],
      "metadata": {
        "id": "O0Cq3HUjO84M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavLavaVisualModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility =\n",
        "  minNavAbility =\n",
        "  minVAcuityAbility =\n",
        "\n",
        "  maxPermAbility =\n",
        "  maxNavAbility =\n",
        "  maxVAcuityAbility =\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility =\n",
        "\n",
        "    navAbility =\n",
        "\n",
        "\n",
        "    vAcuityAbility =\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDistNavigation = pm.MutableData(\"goalDistanceNavigation\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "    goalDistanceVisual  = pm.MutableData(\"goalDistanceVisual\", data[\"euclideanDistanceGoal\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin =\n",
        "    objPermP =\n",
        "\n",
        "    navP =\n",
        "\n",
        "    lavaP =\n",
        "\n",
        "    visualAcuityP =\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP =\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "Xl7gG2UpNwJj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title A Recommended Solution\n",
        "def setupOPNavLavaVisualModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).min())\n",
        "  minVAcuityAbility = ((data[\"euclideanDistanceGoal\"]/data[\"mainGoalSize\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).max())\n",
        "  maxVAcuityAbility = ((data[\"euclideanDistanceGoal\"]/data[\"mainGoalSize\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1, 1)\n",
        "\n",
        "    vAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minVAcuityAbility, maxVAcuityAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDistNavigation = pm.MutableData(\"goalDistanceNavigation\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "    goalDistanceVisual  = pm.MutableData(\"goalDistanceVisual\", data[\"euclideanDistanceGoal\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - ((goalDistNavigation* opTest) * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999((navAbility - (goalDistNavigation * numTurnsGoal)), min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", SimplePrMargin(lavaAbility, lavaPresence))\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic999((np.log(vAcuityAbility) - np.log(goalDistanceVisual/goalSize)), min = minVAcuityAbility, max = maxVAcuityAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP * lavaP * visualAcuityP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "SzOfIjjwPgTk",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavLavaVisualModel(opiaagets_dataset, 'Random_Agent_Success')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "Hbjub6JZRUnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Play around with the estimates for the different agents:"
      ],
      "metadata": {
        "id": "rwdoEJXMpFod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, abilityMin, abilityMax = setupOPNavLavaVisualModel(opiaagets_dataset, 'Dreamer_Success')\n",
        "with model:\n",
        "  idata = pm.sample(1000, tune = 500, target_accept=0.95, random_seed=rng)\n"
      ],
      "metadata": {
        "id": "X-OSGlXxRd6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['objPermAbility']])\n",
        "axes = forest_plot.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n"
      ],
      "metadata": {
        "id": "6GuUe237Rntm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['navAbility']])\n",
        "axes = forest_plot.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "pDgPZwVWRs82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['lavaAbility']])\n",
        "axes = forest_plot.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['lavaAbility'], right=abilityMax['lavaAbility'])"
      ],
      "metadata": {
        "id": "gKxRo5uFRtWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending To The Multivariate Case\n",
        "\n",
        "The test battery has been extended to incorporate another kind of object permanence test, as well as another response variable: whether the agent chose the correct choice or not. In tasks where there was no choice to be made, the correct choice is simply whether they succeeded on the task."
      ],
      "metadata": {
        "id": "N7r0v3mI_kcT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's introduce the second response variable, `correctChoice`, and the corresponding metafeatures. There is the navigation demand of navigating to the goal, which is quite high in both the PCTB and CV Chick tasks. However, there is also the navigation demand of navigating to the choice point, which is high for the PCTB tasks (equivalent to navigating to the goal), but low for the CV Chick tasks (they only need to navigate to the end of the ramp). Find a possible implementation below:\n",
        "\n",
        "Feel free to play around with how everything is connected:"
      ],
      "metadata": {
        "id": "B7RV57eMKFo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupMultivariateModel(data, agent_success_name: str, agent_choice_name: str):\n",
        "\n",
        "  # get results column\n",
        "  successes = data[agent_success_name]\n",
        "  choices = data[agent_choice_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minSuccessNav = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).min())\n",
        "  minChoiceNav = ((data[\"navigationDistanceChoice\"] * data[\"navigationTurnsChoice\"]).min())\n",
        "\n",
        "  maxSuccessNav = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).max())\n",
        "  maxChoiceNav = ((data[\"navigationDistanceChoice\"] * data[\"navigationTurnsChoice\"]).max())\n",
        "\n",
        "  minPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = min([minSuccessNav, minChoiceNav])\n",
        "  minVAcuityAbility = ((data[\"euclideanDistanceGoal\"]/data[\"mainGoalSize\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = max([maxSuccessNav, maxChoiceNav])\n",
        "  maxVAcuityAbility = ((data[\"euclideanDistanceGoal\"]/data[\"mainGoalSize\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    vAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minVAcuityAbility, maxVAcuityAbility)\n",
        "\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1, 1)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistanceNavigation\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "    goalDistanceVisual  = pm.MutableData(\"goalDistanceVisual\", data[\"euclideanDistanceGoal\"].values)\n",
        "\n",
        "    choiceDist = pm.MutableData(\"goalDistanceChoice\", data[\"navigationDistanceGoal\"].values)\n",
        "    numTurnsChoice = pm.MutableData(\"navigationTurnsChoice\", data[\"navigationTurnsChoice\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMarginSuccess = (objPermAbility - ((goalDist * opTest) * numChoices))\n",
        "    objPermSuccessP = pm.Deterministic(\"objPermSuccessP\", logistic999(objPermMarginSuccess, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    objPermMarginChoice = (objPermAbility - ((choiceDist * opTest) * numChoices ))\n",
        "    objPermChoiceP = pm.Deterministic(\"objPermChoiceP\", logistic999(objPermMarginChoice, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(lavaAbility - lavaPresence, min = 0, max = 1))\n",
        "\n",
        "    navSuccessP = pm.Deterministic(\"navSuccessP\", logistic999((navAbility - (goalDist * numTurnsGoal)), min = minNavAbility, max = maxNavAbility) * lavaP)\n",
        "    navChoiceP = pm.Deterministic(\"navChoiceP\", logistic999((navAbility - (choiceDist * numTurnsChoice)), min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic999((np.log(vAcuityAbility) - np.log(goalDistanceVisual/goalSize)), min = minVAcuityAbility, max = maxVAcuityAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    choiceP = pm.Deterministic(\"choiceP\", (navChoiceP * objPermChoiceP * visualAcuityP))\n",
        "    successP = pm.Deterministic(\"successP\", (navSuccessP * objPermSuccessP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", successP, observed=successes)\n",
        "    taskChoice = pm.Bernoulli(\"CorrectChoice\", choiceP, observed=choices)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "eQpUG7kSK9pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupMultivariateModel(opiaagets_dataset, 'Random_Agent_Success', 'Random_Agent_Choice')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "Qnbpz5_XNaff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here's the most complex measurement layout, with the most capabilities, that we can build with the data available. Have a play around with the different connections:"
      ],
      "metadata": {
        "id": "v0tdAiGqpyjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupComplexMultivariateModel(data, agent_success_name: str, agent_choice_name: str):\n",
        "\n",
        "  # get results column\n",
        "  successes = data[agent_success_name]\n",
        "  choices = data[agent_choice_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minSuccessNav = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).min())\n",
        "  minChoiceNav = ((data[\"navigationDistanceChoice\"] * data[\"navigationTurnsChoice\"]).min())\n",
        "\n",
        "  maxSuccessNav = ((data[\"navigationDistanceGoal\"] * data[\"navigationTurnsGoal\"]).max())\n",
        "  maxChoiceNav = ((data[\"navigationDistanceChoice\"] * data[\"navigationTurnsChoice\"]).max())\n",
        "\n",
        "  minPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = min([minSuccessNav, minChoiceNav])\n",
        "  minVAcuityAbility = ((data[\"navigationDistanceGoal\"]/data[\"mainGoalSize\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"navigationDistanceGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = max([maxSuccessNav, maxChoiceNav])\n",
        "  maxVAcuityAbility = ((data[\"navigationDistanceGoal\"]/data[\"mainGoalSize\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  abilityMin[\"rightAbility\"] = 0\n",
        "  abilityMax[\"rightAbility\"] = 1\n",
        "\n",
        "  abilityMin[\"leftAbility\"] = 0\n",
        "  abilityMax[\"leftAbility\"] = 1\n",
        "\n",
        "  abilityMin[\"aheadAbility\"] = 0\n",
        "  abilityMax[\"aheadAbility\"] = 1\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    vAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minVAcuityAbility, maxVAcuityAbility)\n",
        "\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1, 1)\n",
        "\n",
        "    rightAbility = pm.Beta(\"rightAbility\", 1, 1)\n",
        "\n",
        "    centreAbility = pm.Beta(\"centreAbility\", 1, 1)\n",
        "\n",
        "    leftAbility = pm.Beta(\"leftAbility\", 1, 1)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistanceNavigation\", data[\"navigationDistanceGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"navigationTurnsGoal\", data[\"navigationTurnsGoal\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "    #goalDistanceVisual  = pm.MutableData(\"goalDistanceVisual\", data[\"euclideanDistanceGoal\"].values)\n",
        "\n",
        "    choiceDist = pm.MutableData(\"goalDistanceChoice\", data[\"navigationDistanceGoal\"].values)\n",
        "    numTurnsChoice = pm.MutableData(\"navigationTurnsChoice\", data[\"navigationTurnsChoice\"].values)\n",
        "\n",
        "    goalRight = pm.MutableData(\"goalRight\", data[\"goalRight\"].values)\n",
        "    goalCentre = pm.MutableData(\"goalCentre\", data[\"goalCentre\"].values)\n",
        "    goalLeft = pm.MutableData(\"goalLeft\", data[\"goalLeft\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMarginSuccess = (objPermAbility - ((goalDist * opTest) * numChoices))\n",
        "    objPermSuccessP = pm.Deterministic(\"objPermSuccessP\", logistic999(objPermMarginSuccess, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    objPermMarginChoice = (objPermAbility - ((choiceDist * opTest) * numChoices ))\n",
        "    objPermChoiceP = pm.Deterministic(\"objPermChoiceP\", logistic999(objPermMarginChoice, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", SimplePrMargin(lavaAbility, lavaPresence))\n",
        "\n",
        "    rightP = pm.Deterministic(\"rightP\", SimplePrMargin(rightAbility, goalRight))\n",
        "    centreP = pm.Deterministic(\"centreP\", SimplePrMargin(centreAbility, goalCentre))\n",
        "    leftP = pm.Deterministic(\"leftP\", SimplePrMargin(leftAbility, goalLeft))\n",
        "\n",
        "    navSuccessP = pm.Deterministic(\"navSuccessP\", logistic999((navAbility - (goalDist * numTurnsGoal)), min = minNavAbility, max = maxNavAbility) * lavaP)\n",
        "    navChoiceP = pm.Deterministic(\"navChoiceP\", logistic999((navAbility - (choiceDist * numTurnsChoice)), min = minNavAbility, max = maxNavAbility) * rightP * centreP * leftP)\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic999((np.log(vAcuityAbility) - np.log(goalDist/goalSize)), min = minVAcuityAbility, max = maxVAcuityAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    choiceP = pm.Deterministic(\"choiceP\", (navChoiceP * objPermChoiceP ))\n",
        "    successP = pm.Deterministic(\"successP\", (navSuccessP * objPermSuccessP * visualAcuityP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"Success\", successP, observed=successes)\n",
        "    taskChoice = pm.Bernoulli(\"CorrectChoice\", choiceP, observed=choices)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "ynHw0jg1Zcqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupComplexMultivariateModel(opiaagets_dataset, 'Random_Agent_Success', 'Random_Agent_Choice')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "Q0eEXK_odgWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try fitting it to an agent:"
      ],
      "metadata": {
        "id": "xzK-adfaqBbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, abilityMin, abilityMax = setupComplexMultivariateModel(human_dataset, 'Human_Success', 'Human_Choice')\n",
        "with model:\n",
        "  idata = pm.sample(1000, tune = 500, target_accept=0.95, random_seed=rng)\n"
      ],
      "metadata": {
        "id": "yJ5vNVEeqAlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And plotting some of its capabilities:"
      ],
      "metadata": {
        "id": "nebQ5MLsqLdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot = az.plot_forest(data=idata['posterior'][['objPermAbility']])\n",
        "axes = forest_plot.ravel()[0]\n",
        "axes.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "pS3mrpRZqLvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BONUS\n",
        "\n",
        "If there is time at the end:\n",
        "\n",
        "1. Try running some diagnostics on the fitted models:"
      ],
      "metadata": {
        "id": "PIlznPrwqSgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(idata['posterior'][['objPermAbility']])\n",
        "az.plot_energy(idata)\n",
        "az.summary(idata['posterior'][['objPermAbility', 'navAbility', 'visualAcuityAbility']])"
      ],
      "metadata": {
        "id": "GCW_hFurqa_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Play around with the models to try to improve their fit (they're works-in-progress!) and predictiveness.\n",
        "3. Build a large multi-level measurement layout for running inferences on multiple agents simultaneously. A place to start for implementing this would be [here](https://www.pymc.io/projects/examples/en/latest/generalized_linear_models/multilevel_modeling.html)."
      ],
      "metadata": {
        "id": "9p6PUVNSqnVm"
      }
    }
  ]
}