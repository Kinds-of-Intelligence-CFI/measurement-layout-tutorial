{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUN1w8q3/GQRoEW4tuH8Ak",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/blob/main/tutorial-notebooks/4_BuildingGoodBenchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Good Benchmarks\n",
        "\n",
        "**Lead Presenter**: Kozzy Voudouris\n",
        "\n",
        "In this tutorial, we introduce some key concepts that should guide the development of purpose-built benchmarks for use with measurement layouts. In this notebook, we incrementally build a measurement layout for studying the cognitive capability of object permanence in a complex three-dimensional environment. Finally, We evaluate some agents on a suite of tests."
      ],
      "metadata": {
        "id": "EOQkgyhAQMZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preamble\n",
        "\n",
        "First, let's import the libraries, functions, and data that we will need."
      ],
      "metadata": {
        "id": "uuPrpVEizOiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arviz --quiet\n",
        "!pip install erroranalysis --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip install pymc --quiet"
      ],
      "metadata": {
        "id": "e-_FrQEczOLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import erroranalysis as ea\n",
        "import gc\n",
        "import graphviz\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import random as rm\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "from pymc import model\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\") #Note, colab imports an older version of PyMC by default. This won't cause problems for this tutorial, but may do if you use a different backend (e.g., gpu) and a jax/numpyro sampler. In which case, run `!pip install 'pymc>5.9' --quiet`"
      ],
      "metadata": {
        "id": "nNjpBg-Pzu5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/4_PCTB_data.csv'\n",
        "pctb_dataset = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "e-D77_5w0O-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the dataset. There are **551 instances**, **8 metafeatures**, and the performances of **7 agents**.\n",
        "\n",
        "The metafeatures are as follows:\n",
        "1. `basicTask` - is the task a basic task? Values (discrete, binary): `0` (No), `1` (Yes).\n",
        "2. `pctbGridTask` - is the task a PCTB Grid task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "3. `mainGoalSize` - what size is the goal in the task? Value range: `0.5` - `5.0`.\n",
        "4. `goalPosition` - what is the relative position of the goal with respect to the agent's starting position? Value range: `-1.5` - `1.5` (0 is centre point).\n",
        "5. `goalOccluded` - is the goal occluded when the agent starts the episode? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "6. `minDistToGoal` - how far is the goal from the agent? This is calculated is the manhattan distance to the goal, avoiding any obstacles/pits. Value range: `9.0` - `47.0`.\n",
        "7. `minNumTurnsGoal` - how many right-angle turns would the agent take on the trajectory described by `minDistToGoal`. Value range: `0.0` - `3.0`.\n",
        "8. `numChoices` - how many choices does the agent have in the task? Value range: `1.0` - `12.0`.\n",
        "\n",
        "The agents performed each of these tasks, and whether they obtained the goal (`1`) or not (`0`) was recorded. The agents are as follows:\n",
        "1. `Random_Agent` - An agent which randomly samples one of the 9 actions in the Animal-AI Environment (`no action`, `forwards`, `backwards`, `left rotate`, `right rotate`, `forwards left`, `forwards right`, `backwards left`, `backwards right`). It then takes that action for a number of steps sampled from $U(1, 20)$.\n",
        "2. `Heuristic_Agent` - An agent that navigates towards green goals, following a rigid rule.\n",
        "3. `Dreamer_basic` - A dreamer-v3 agent trained for 2M steps on a set of 300 basic tasks, of which all tasks where `basicTask == 1` are a subset.\n",
        "4. `Dreamer_basic_control` - A dreamer-v3 agent trained for 10M steps on a set of 2372 basic and practice tasks, of which all tasks where `basicTask == 1 or (pctbGridTask == 1 and goalOccluded == 0)` are a subset.\n",
        "5. `PPO_basic` - A PPO agent trained for 2M steps on a set of 300 basic tasks, of which all tasks where `basicTask == 1` are a subset.\n",
        "6. `PPO_basic_control` - A PPO agent trained for 10M steps on a set of 2372 basic and practice tasks, of which all tasks where `basicTask == 1 or (pctbGridTask == 1 and goalOccluded == 0)` are a subset."
      ],
      "metadata": {
        "id": "EH9a5LS60luD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pctb_dataset"
      ],
      "metadata": {
        "id": "sbsoI-vN0j38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Complex Measurement Layout\n",
        "\n",
        "We will incrementally build a complex measurement layout for studying object permanence. We will evaluate this measurement layout by using the two reference agents, `Random_Agent` and `Heuristic_Agent`, as baselines. Finally, we will apply this measurement layout to the dreamer and PPO agents.\n",
        "\n",
        "## Object Permanence\n",
        "\n",
        "First, let's set up a simple measurement layout for the *Object Permanence* ability.\n",
        "\n",
        "The leaf node of the measurement layout is success in this case, so we want to define it as a Bernoulli, taking a probability of success. Therefore, we'll need a logistic function. Because we are dealing with bounded capabilities, the logistic function means that the probability of success on a task with a minimum demand for an agent with maximum ability is 0.999. Alternatively, for an agent with minimum ability performing on a task with maximum demand, we get a probability of success of 0.001. This is a nice parameterisation of the logistic function for our case of bounded capabilities."
      ],
      "metadata": {
        "id": "jR5RHB45zKpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic999(x, min, max):    # This logistic function ensures that if x is at -(max-min), we get prob 0.001, and if x is at (max-min), we get prob 0.999\n",
        "  x = x - min\n",
        "  max = max - min\n",
        "  x = 6.90675478 * x / max\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "3eNXs1trEy4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTWHzmLlQHfK"
      },
      "outputs": [],
      "source": [
        "def setupOPModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", objPermP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPModel(pctb_dataset, 'Random_Agent')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "rzjj9VM0GIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this measurement layout on two of our agents: the `Random_Agent` agent and the `Heuristic_Agent`."
      ],
      "metadata": {
        "id": "PJtPtgtqI970"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPModel(pctb_dataset, 'Random_Agent')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPModel(pctb_dataset, 'Heuristic_Agent')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "8s0nYG1FGvDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the inferred object permanence capability of these agents, by plotting their capabilities as forest plots:"
      ],
      "metadata": {
        "id": "edXKGnXHJPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['objPermAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "52tC1XRwHNKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at the summary statistics from the two measurement layouts:"
      ],
      "metadata": {
        "id": "-pn-N5ZzJ1Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_random = az.summary(data_random['posterior']['objPermAbility'])\n",
        "summary_random"
      ],
      "metadata": {
        "id": "PXUlQ2h6Hd37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_heuristic = az.summary(data_heuristic['posterior']['objPermAbility'])\n",
        "summary_heuristic"
      ],
      "metadata": {
        "id": "a9DVFTaKJ-YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are inferring that the heuristic agent has higher object permanence than the random agent, but both agents are relatively low on object permanence."
      ],
      "metadata": {
        "id": "9Qy1wpYQKDcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Navigation\n",
        "\n",
        "These tasks are fundamentally search tasks. The agent must navigate towards the reward. As such, an agent with object permanence but poor at navigation may fail many of these tasks. Moreover, an agent without object permanence, but that is good at navigating, may accidentally obtain the reward on occasion.\n",
        "\n",
        "The simplest way to frame this is to say that navigation and object permanence are non-compensatory - being good at navigation does not compensate for being bad at object permanence, and vice versa. For the purposes of this tutorial, we can proceed with this formulation, although it may be more accurate to implement an asymmetric compensatory relationship between these too (since, arguably, navigation is more compensatory for object permanence than vice versa).\n",
        "\n",
        "Navigation demands can be implemented in terms of how far away the goal is along with the circuitousness of the route. We can define this as the product of distance and number of turns.\n",
        "\n",
        "Let's extend the measurement layout to include navigation:"
      ],
      "metadata": {
        "id": "ILpMtAGVKVS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", data[\"minNumTurnsGoal\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999(navAbility - (goalDist * numTurnsGoal), min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "Eai-I-daRPXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Random_Agent')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "BEcPZlisSVdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this new measurement layout with the same two agents:"
      ],
      "metadata": {
        "id": "z6D4NpKBSpJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Random_Agent')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Heuristic_Agent')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "h3JUouDISozT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the inferred object permanence and navigation capabilities of these agents, by plotting their capabilities as forest plots:"
      ],
      "metadata": {
        "id": "r0XTu2k3S5EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['objPermAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "iEnF2gJqSw2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['navAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['navAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "U0dY9fdMTEBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cognitive profile for the heuristic agent is as expected: high navigation capability, low object permanence capability. However, the random agent is not as expected: we are inferring a high object permanence capability (albeit with very wide credibility intervals). In fact, navigation is more complex than simply the distance to the goal combined with how circuitous the route is. Perhaps there is evidence of a bias towards goals in certain directions. We can implement this next:\n",
        "\n",
        "## Introducing Directional Bias\n",
        "\n",
        "The position of the goal relative to the agent's starting position is encoded as a float centred at 0, with left of centre being negative."
      ],
      "metadata": {
        "id": "hViNiItlFC6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavDirectionalModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"rightLeftBias\"] = -np.inf\n",
        "  abilityMax[\"rightLeftBias\"] = np.inf\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    rightLeftBias = pm.Normal(\"rightLeftBias\", 0,1)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", data[\"minNumTurnsGoal\"].values)\n",
        "    rightLeftPosition = pm.MutableData(\"rightLeftPosition\", data[\"goalPosition\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    rightLeftEffect = pm.Deterministic(\"rightLeftEffect\", rightLeftBias * rightLeftPosition)\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999((navAbility - (goalDist * numTurnsGoal)) + rightLeftEffect, min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "CkeH2abEGFk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavDirectionalModel(pctb_dataset, 'Random_Agent')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "BNqjEQIUIK7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPNavDirectionalModel(pctb_dataset, 'Random_Agent')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPNavDirectionalModel(pctb_dataset, 'Heuristic_Agent')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "XGqgAABrIf9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['objPermAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "grcbGYk-IQqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['navAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['navAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "vg4lMsjBJSKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['rightLeftBias']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=-3, right=3)\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['rightLeftBias']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=-3, right=3)"
      ],
      "metadata": {
        "id": "5v30rKZOJXbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The estimated object permanence capability for the random agent has been pulled down slightly, but it is still curiously high. Perhaps visual acuity can explain some of the extra variance in performance.\n",
        "\n",
        "## Introducing Visual Acuity\n",
        "\n",
        "As in the previous tutorial, we define visual acuity in terms of the size and distance of the goal (when it is visible) from the agent at the start of the episode."
      ],
      "metadata": {
        "id": "O0Cq3HUjO84M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavDirectionalVisualModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).min())\n",
        "  minVAcuityAbility = ((data[\"minDistToGoal\"]/data[\"mainGoalSize\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).max())\n",
        "  maxVAcuityAbility = ((data[\"minDistToGoal\"]/data[\"mainGoalSize\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"rightLeftBias\"] = -np.inf\n",
        "  abilityMax[\"rightLeftBias\"] = np.inf\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    rightLeftBias = pm.Normal(\"rightLeftBias\", 0,1)\n",
        "\n",
        "    vAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minVAcuityAbility, maxVAcuityAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", data[\"minNumTurnsGoal\"].values)\n",
        "    rightLeftPosition = pm.MutableData(\"rightLeftPosition\", data[\"goalPosition\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    rightLeftEffect = pm.Deterministic(\"rightLeftEffect\", rightLeftBias * rightLeftPosition)\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999((navAbility - (goalDist * numTurnsGoal)) + rightLeftEffect, min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic999((np.log(vAcuityAbility) - np.log(goalDist/goalSize)), min = minVAcuityAbility, max = maxVAcuityAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP * visualAcuityP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "SzOfIjjwPgTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'Random_Agent')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "Hbjub6JZRUnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'Random_Agent')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'Heuristic_Agent')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "X-OSGlXxRd6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['objPermAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "6GuUe237Rntm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['navAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['navAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "pDgPZwVWRs82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['rightLeftBias']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=-3, right=3)\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['rightLeftBias']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=-3, right=3)"
      ],
      "metadata": {
        "id": "gKxRo5uFRtWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['visualAcuityAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['visualAcuityAbility'], right=abilityMax['visualAcuityAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['visualAcuityAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['visualAcuityAbility'], right=abilityMax['visualAcuityAbility'])"
      ],
      "metadata": {
        "id": "mB4C50uNRxQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing visual acuity tidies a lot of things up. We get a cognitive profile that we expect for both agents. Both have quite good navigation abilities, low object permanence, and some differences in the left/right bias and visual acuities."
      ],
      "metadata": {
        "id": "oIAo5iG_SZsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making Inferences About Real Agents\n",
        "\n",
        "Let's now run this measurement layout on our four DRL agents, and then inspect some diagnostics to determine how well the model is fitting."
      ],
      "metadata": {
        "id": "t5KFYm1GVKre"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_ppo_basic, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'PPO_basic')\n",
        "with model_ppo_basic:\n",
        "  data_ppo_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_ppo_basic_control, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'PPO_basic_control')\n",
        "with model_ppo_basic_control:\n",
        "  data_ppo_basic_control = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_dreamer_basic, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'Dreamer_basic')\n",
        "with model_dreamer_basic:\n",
        "  data_dreamer_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_dreamer_basic_control, abilityMin, abilityMax = setupOPNavDirectionalVisualModel(pctb_dataset, 'Dreamer_basic_control')\n",
        "with model_dreamer_basic_control:\n",
        "  data_dreamer_basic_control = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "GJDvlNAsVVCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def capability_forest_plot(capability_idata, ax_min, ax_max):\n",
        "  \"\"\"\n",
        "  capability_idata : the posterior for the capability\n",
        "  ax_min : the minimum value on the x axis for the forest plot\n",
        "  ax_max = the maximum value on the x axis for the forest plot\n",
        "  \"\"\"\n",
        "  forest_plot = az.plot_forest(data = capability_idata)\n",
        "  axes = forest_plot.ravel()[0]\n",
        "  axes.set_xlim(left = ax_min, right = ax_max)\n",
        "\n",
        "  return None"
      ],
      "metadata": {
        "id": "wVR2TYDGfPHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dreamer agents have slightly higher object permanence than the PPO agents:"
      ],
      "metadata": {
        "id": "GGUT4B-jhD1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capability_forest_plot(data_ppo_basic['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_ppo_basic_control['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_dreamer_basic['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_dreamer_basic_control['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "IorikX4XfCVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They all have high navigation abilities:"
      ],
      "metadata": {
        "id": "TsyvIRwAhQeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capability_forest_plot(data_ppo_basic['posterior'][['navAbility']], ax_min = abilityMin['navAbility'], ax_max = abilityMax['navAbility'])\n",
        "capability_forest_plot(data_ppo_basic_control['posterior'][['navAbility']], ax_min = abilityMin['navAbility'], ax_max = abilityMax['navAbility'])\n",
        "capability_forest_plot(data_dreamer_basic['posterior'][['navAbility']], ax_min = abilityMin['navAbility'], ax_max = abilityMax['navAbility'])\n",
        "capability_forest_plot(data_dreamer_basic_control['posterior'][['navAbility']], ax_min = abilityMin['navAbility'], ax_max = abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "rIhA72yRhHda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "They are not particularly biased to one side:"
      ],
      "metadata": {
        "id": "dk1Ua6TBhoUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capability_forest_plot(data_ppo_basic['posterior'][['rightLeftBias']], ax_min = -3, ax_max = 3)\n",
        "capability_forest_plot(data_ppo_basic_control['posterior'][['rightLeftBias']], ax_min = -3, ax_max = 3)\n",
        "capability_forest_plot(data_dreamer_basic['posterior'][['rightLeftBias']], ax_min = -3, ax_max = 3)\n",
        "capability_forest_plot(data_dreamer_basic_control['posterior'][['rightLeftBias']], ax_min = -3, ax_max = 3)"
      ],
      "metadata": {
        "id": "vIkt6HaNhP6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visual acuity of dreamer is higher than that of PPO, but not significantly so. Both agents received the same 64x64 RGB pixel input."
      ],
      "metadata": {
        "id": "moSTc9d5hsqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "capability_forest_plot(data_ppo_basic['posterior'][['visualAcuityAbility']], ax_min = abilityMin['visualAcuityAbility'], ax_max = abilityMax['visualAcuityAbility'])\n",
        "capability_forest_plot(data_ppo_basic_control['posterior'][['visualAcuityAbility']], ax_min = abilityMin['visualAcuityAbility'], ax_max = abilityMax['visualAcuityAbility'])\n",
        "capability_forest_plot(data_dreamer_basic['posterior'][['visualAcuityAbility']], ax_min = abilityMin['visualAcuityAbility'], ax_max = abilityMax['visualAcuityAbility'])\n",
        "capability_forest_plot(data_dreamer_basic_control['posterior'][['visualAcuityAbility']], ax_min = abilityMin['visualAcuityAbility'], ax_max = abilityMax['visualAcuityAbility'])"
      ],
      "metadata": {
        "id": "ME_hdiN7hsQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Diagnostics\n",
        "\n",
        "PyMC is powerful and intuitive enough that we can straightforwardly design measurement layouts. But how do we know if the data are appropriate for them, or whether there are any issues with how they were fitted? There are several diagnostics we can run on measurement layouts, of which some of the most useful are presented here.\n",
        "\n",
        "First, we can look at the traces for each of the capabilities. Below, we can visualise the traces. For each chain (we have used 2 chains above), there is a posterior distribution (see the left plot). We want them to look relatively similar to each other, as this means that each chain converged to a similar posterior. On the right, we see a time series plot indicating how often each of the values were sampled in the chain. We want there to be relative homogeneity here, suggesting that all values were sampled a similar number of times. Note, depending on the prior, there might be spikes for certain values (if, for instance, the prior is a Cauchy distribution or something similarly heavy-tailed)."
      ],
      "metadata": {
        "id": "RPRf8xwVirKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(data_ppo_basic['posterior'][['objPermAbility']])\n",
        "az.plot_trace(data_ppo_basic_control['posterior'][['objPermAbility']])\n",
        "az.plot_trace(data_dreamer_basic['posterior'][['objPermAbility']])\n",
        "az.plot_trace(data_dreamer_basic_control['posterior'][['objPermAbility']])"
      ],
      "metadata": {
        "id": "q279UGfDiqzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(data_ppo_basic['posterior'][['navAbility']])\n",
        "az.plot_trace(data_ppo_basic_control['posterior'][['navAbility']])\n",
        "az.plot_trace(data_dreamer_basic['posterior'][['navAbility']])\n",
        "az.plot_trace(data_dreamer_basic_control['posterior'][['navAbility']])"
      ],
      "metadata": {
        "id": "YcVaXQ6PmA5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(data_ppo_basic['posterior'][['rightLeftBias']])\n",
        "az.plot_trace(data_ppo_basic_control['posterior'][['rightLeftBias']])\n",
        "az.plot_trace(data_dreamer_basic['posterior'][['rightLeftBias']])\n",
        "az.plot_trace(data_dreamer_basic_control['posterior'][['rightLeftBias']])"
      ],
      "metadata": {
        "id": "KKKkyDKhmCxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(data_ppo_basic['posterior'][['visualAcuityAbility']])\n",
        "az.plot_trace(data_ppo_basic_control['posterior'][['visualAcuityAbility']])\n",
        "az.plot_trace(data_dreamer_basic['posterior'][['visualAcuityAbility']])\n",
        "az.plot_trace(data_dreamer_basic_control['posterior'][['visualAcuityAbility']])"
      ],
      "metadata": {
        "id": "Vcpyeg5cmDK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A second diagnostic is an energy plot, which also enables us to check whether the MCMC algorithm (usually, as here, NUTS) has explored the full posterior distribution. In the energy plot, we simply want the distribution of marginal energy during sampling, and distribution of energy transitions between steps (see [here](https://www.pymc.io/projects/docs/en/latest/learn/core_notebooks/pymc_overview.html#model-checking) and [here](https://arxiv.org/abs/1604.00695)), to overlap and look similar. Everything looks good for our four agents with our complex measurement layout:"
      ],
      "metadata": {
        "id": "amyzbjw7nH57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_energy(data_ppo_basic)\n",
        "az.plot_energy(data_ppo_basic_control)\n",
        "az.plot_energy(data_dreamer_basic)\n",
        "az.plot_energy(data_dreamer_basic_control)"
      ],
      "metadata": {
        "id": "nEmNrk9zoKaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the literature on Bayesian statistics, several convergence diagnostics have been proposed. [Vehtari et al. (2021)](https://projecteuclid.org/journals/bayesian-analysis/volume-16/issue-2/Rank-Normalization-Folding-and-Localization--An-Improved-R%cb%86-for/10.1214/20-BA1221.full) present a comprehensive overview. Two diagnostics that we can use out of the box to gauge the convergence of multiple chains when fitting a measurement layout are $\\hat{R}$ and *Effective Sample Size* (ESS).\n",
        "\n",
        "$\\hat{R}$ is, roughly, the ratio of the variance mixed across all chains compared to the root mean squared variance of the variance in each individual chain. If the chains are not converging, then the between-chain variance should be higher than the within-chain variance, so values higher than 1 indicate lack of convergence. In practice, Vehtari et al. suggest values higher than 1.01 indicate a lack of convergence. Below, we present $\\hat{R}$ for each capability.\n",
        "\n",
        "ESS is, roughly, \"how many independent draws contain the same information as the dependent sample obtained by the MCMC algorithm. The higher the ESS the better\" (Vehtari et al., 2021, p. 672). We can distinguish between $ESS_{bulk}$ and $ESS_{tail}$ too, where the latter is the ESS in the tails of the posterior distribution, outside of the credibility interval. This is especially useful if credibility intervals are to be used downstream in inference. Below, note that the sample size for each chain is 2000 (1000 warm up, 1000 sample; unless you have changed it)."
      ],
      "metadata": {
        "id": "ehptu-07ovI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az.summary(data_ppo_basic['posterior'][['objPermAbility', 'navAbility', 'rightLeftBias', 'visualAcuityAbility']])"
      ],
      "metadata": {
        "id": "93b37EABxuz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.summary(data_ppo_basic_control['posterior'][['objPermAbility', 'navAbility', 'rightLeftBias', 'visualAcuityAbility']])"
      ],
      "metadata": {
        "id": "95CA1ySOyQp0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.summary(data_dreamer_basic['posterior'][['objPermAbility', 'navAbility', 'rightLeftBias', 'visualAcuityAbility']])"
      ],
      "metadata": {
        "id": "HjrWSoS7ySmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.summary(data_dreamer_basic_control['posterior'][['objPermAbility', 'navAbility', 'rightLeftBias', 'visualAcuityAbility']])"
      ],
      "metadata": {
        "id": "Gzit6oZRySRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extending To The Multivariate Case\n",
        "\n",
        "The test battery has been extended to incorporate another kind of object permanence test, as well as another response variable: whether the agent chose the correct choice or not.\n",
        "\n",
        "Let's inspect the dataset. There are **551 instances**, **8 metafeatures**, and the performances of **7 agents** across two measures.\n",
        "\n",
        "The metafeatures are as follows:\n",
        "1. `basicTask` - is the task a basic task? Values (discrete, binary): `0` (No), `1` (Yes).\n",
        "2. `pctbGridTask` - is the task a PCTB Grid task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "3. `cvChickTask` - is the task a CV Chick task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "4. `mainGoalSize` - what size is the goal in the task? Value range: `0.5` - `5.0`.\n",
        "5. `goalPosition` - what is the relative position of the goal with respect to the agent's starting position? Value range: `-1.5` - `1.5` (0 is centre point).\n",
        "6. `goalOccluded` - is the goal occluded when the agent starts the episode? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "7. `lavaPresence` - is there lava in the arena? Values (discrete, binary): `0` (No), `1` (Yes).\n",
        "8. `minDistToGoal` - how far is the goal from the agent? This is calculated is the manhattan distance to the goal, avoiding any obstacles/pits. Value range: `9.0` - `58.0`.\n",
        "9. `minNumTurnsGoal` - how many right-angle turns would the agent take on the trajectory described by `minDistToGoal`. Value range: `0.0` - `11.0`.\n",
        "10. `minDistToCorrectChoice` - how far is the choice point from the agent? This is calculated is the manhattan distance to the goal, avoiding any obstacles/pits. For tasks without a forced choice component, it is equivalent to the distance to the goal.  Value range: `8.0` - `47.0`.\n",
        "11. `minNumTurnsChoice` - how many right-angle turns would the agent take on the trajectory described by `minDistToCorrectChoice`. Value range: `0.0` - `3.0`.\n",
        "12. `numChoices` - how many choices does the agent have in the task? Value range: `1.0` - `12.0`.\n",
        "\n",
        "The agents performed each of these tasks, and whether they obtained the goal (`1`) or not (`0`) was recorded, as well as whether they chose the correct choice in a forced choice task (`1`) or not (`0`). For tasks where there was no choice to be made, this value was equivalent to whether they succeeded on the task. There are two columns for each agent, `*_Success` and `*_Choice`. The agents are as follows:\n",
        "1. `Random_Agent` - An agent which randomly samples one of the 9 actions in the Animal-AI Environment (`no action`, `forwards`, `backwards`, `left rotate`, `right rotate`, `forwards left`, `forwards right`, `backwards left`, `backwards right`). It then takes that action for a number of steps sampled from $U(1, 20)$.\n",
        "2. `Heuristic_Agent` - An agent that navigates towards green goals, following a rigid rule.\n",
        "3. `Dreamer_basic` - A dreamer-v3 agent trained for 2M steps on a set of 300 basic tasks, of which all tasks where `basicTask == 1` are a subset.\n",
        "4. `Dreamer_basic_control` - A dreamer-v3 agent trained for 10M steps on a set of 2372 basic and practice tasks, of which all tasks where `basicTask == 1 or (pctbGridTask == 1 and goalOccluded == 0) or (cvchickTask == 1 and goalOccluded == 0)` are a subset.\n",
        "5. `PPO_basic` - A PPO agent trained for 2M steps on a set of 300 basic tasks, of which all tasks where `basicTask == 1` are a subset.\n",
        "6. `PPO_basic_control` - A PPO agent trained for 10M steps on a set of 2372 basic and practice tasks, of which all tasks where `basicTask == 1 or (pctbGridTask == 1 and goalOccluded == 0) or (cvchickTask == 1 and goalOccluded == 0)` are a subset."
      ],
      "metadata": {
        "id": "N7r0v3mI_kcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/4_PCTBCVChick_data.csv'\n",
        "pctbcvchick_dataset = pd.read_csv(data_url)\n",
        "\n",
        "pctbcvchick_dataset"
      ],
      "metadata": {
        "id": "94Rrcrc9DC4s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we have a new metafeature for lava, let's incorporate that into the univariate model. To do so, we will make use of the Beta prior because the `lavaPresence` is a binary metafeature:"
      ],
      "metadata": {
        "id": "dD-yr0rgFb81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavDirectionalVisualLavaModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).min())\n",
        "  minVAcuityAbility = ((data[\"minDistToGoal\"]/data[\"mainGoalSize\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).max())\n",
        "  maxVAcuityAbility = ((data[\"minDistToGoal\"]/data[\"mainGoalSize\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"rightLeftBias\"] = -np.inf\n",
        "  abilityMax[\"rightLeftBias\"] = np.inf\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    rightLeftBias = pm.Normal(\"rightLeftBias\", 0,1)\n",
        "\n",
        "    vAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minVAcuityAbility, maxVAcuityAbility)\n",
        "\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1, 1)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", data[\"minNumTurnsGoal\"].values)\n",
        "    rightLeftPosition = pm.MutableData(\"rightLeftPosition\", data[\"goalPosition\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    rightLeftEffect = pm.Deterministic(\"rightLeftEffect\", rightLeftBias * rightLeftPosition)\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999((navAbility - (goalDist * numTurnsGoal)) + rightLeftEffect, min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic999((np.log(vAcuityAbility) - np.log(goalDist/goalSize)), min = minVAcuityAbility, max = maxVAcuityAbility))\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(lavaAbility - lavaPresence, min = 0, max = 1))\n",
        "\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP * visualAcuityP * lavaP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "EDuHAq09FfsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavDirectionalVisualLavaModel(pctbcvchick_dataset, 'Random_Agent_Success')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "gs4shIoRIs01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try it out on the random agent and see what lava ability is inferred:"
      ],
      "metadata": {
        "id": "8dUu8JXiJfJW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPNavDirectionalVisualLavaModel(pctbcvchick_dataset, 'Random_Agent_Success')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "O_5SpQL_JR21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "az.plot_trace(data_random['posterior'][['lavaAbility']])"
      ],
      "metadata": {
        "id": "4hASOEbHJYyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's introduce the second response variable, `correctChoice`, and the corresponding metafeatures. There is the navigation demand of navigating to the goal, which is quite high in both the PCTB and CV Chick tasks. However, there is also the navigation demand of navigating to the choice point, which is high for the PCTB tasks (equivalent to navigating to the goal), but low for the CV Chick tasks (they only need to navigate to the end of the ramp). Let's implement that:"
      ],
      "metadata": {
        "id": "B7RV57eMKFo-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupMultivariateModel(data, agent_success_name: str, agent_choice_name: str):\n",
        "\n",
        "  # get results column\n",
        "  successes = data[agent_success_name]\n",
        "  choices = data[agent_choice_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minSuccessNav = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).min())\n",
        "  minChoiceNav = ((data[\"minDistToCorrectChoice\"] * data[\"minNumTurnsChoice\"]).min())\n",
        "\n",
        "  maxSuccessNav = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).max())\n",
        "  maxChoiceNav = ((data[\"minDistToCorrectChoice\"] * data[\"minNumTurnsChoice\"]).max())\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = min([minSuccessNav, minChoiceNav])\n",
        "  minVAcuityAbility = ((data[\"minDistToGoal\"]/data[\"mainGoalSize\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = max([maxSuccessNav, maxChoiceNav])\n",
        "  maxVAcuityAbility = ((data[\"minDistToGoal\"]/data[\"mainGoalSize\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "  abilityMin[\"rightLeftBias\"] = -np.inf\n",
        "  abilityMax[\"rightLeftBias\"] = np.inf\n",
        "\n",
        "  abilityMin[\"visualAcuityAbility\"] = minVAcuityAbility\n",
        "  abilityMax[\"visualAcuityAbility\"] = maxVAcuityAbility\n",
        "\n",
        "  abilityMin[\"lavaAbility\"] = 0\n",
        "  abilityMax[\"lavaAbility\"] = 1\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    rightLeftBias = pm.Normal(\"rightLeftBias\", 0,1)\n",
        "\n",
        "    vAcuityAbility = pm.Uniform(\"visualAcuityAbility\", minVAcuityAbility, maxVAcuityAbility)\n",
        "\n",
        "    lavaAbility = pm.Beta(\"lavaAbility\", 1, 1)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", data[\"minNumTurnsGoal\"].values)\n",
        "    rightLeftPosition = pm.MutableData(\"rightLeftPosition\", data[\"goalPosition\"].values)\n",
        "    goalSize = pm.MutableData(\"goalSize\", data[\"mainGoalSize\"].values)\n",
        "    lavaPresence = pm.MutableData(\"lavaPresence\", data[\"lavaPresence\"].values)\n",
        "\n",
        "    choiceDist = pm.MutableData(\"choiceDistance\", data[\"minDistToCorrectChoice\"].values)\n",
        "    numTurnsChoice = pm.MutableData(\"minTurnsToChoice\", data[\"minNumTurnsChoice\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    rightLeftEffect = pm.Deterministic(\"rightLeftEffect\", rightLeftBias * rightLeftPosition)\n",
        "\n",
        "    lavaP = pm.Deterministic(\"lavaP\", logistic999(lavaAbility - lavaPresence, min = 0, max = 1))\n",
        "\n",
        "    navSuccessP = pm.Deterministic(\"navSuccessP\", logistic999((navAbility - (goalDist * numTurnsGoal)), min = minNavAbility, max = maxNavAbility) * lavaP)\n",
        "    navChoiceP = pm.Deterministic(\"navChoiceP\", logistic999((navAbility - (choiceDist * numTurnsChoice)) + rightLeftEffect, min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    visualAcuityP = pm.Deterministic(\"visualAcuityP\", logistic999((np.log(vAcuityAbility) - np.log(goalDist/goalSize)), min = minVAcuityAbility, max = maxVAcuityAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    choiceP = pm.Deterministic(\"choiceP\", (navChoiceP * objPermP))\n",
        "    successP = pm.Deterministic(\"successP\", (visualAcuityP * navSuccessP * objPermP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", successP, observed=successes)\n",
        "    taskChoice = pm.Bernoulli(\"taskChoice\", choiceP, observed=choices)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "eQpUG7kSK9pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'Random_Agent_Success', 'Random_Agent_Choice')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "Qnbpz5_XNaff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this measurement layout with our battery of 6 agents:"
      ],
      "metadata": {
        "id": "QYmdxr1PN71X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'Random_Agent_Success', 'Random_Agent_Choice')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'Heuristic_Agent_Success', 'Heuristic_Agent_Choice')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_ppo_basic, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'PPO_basic_Success', 'PPO_basic_Choice')\n",
        "with model_ppo_basic:\n",
        "  data_ppo_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_ppo_basic, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'PPO_basic_Success', 'PPO_basic_Choice')\n",
        "with model_ppo_basic:\n",
        "  data_ppo_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_ppo_basic_control, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'PPO_basic_control_Success', 'PPO_basic_control_Choice')\n",
        "with model_ppo_basic_control:\n",
        "  data_ppo_basic_control = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_dreamer_basic, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'Dreamer_basic_Success', 'Dreamer_basic_Choice')\n",
        "with model_dreamer_basic:\n",
        "  data_dreamer_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_dreamer_basic_control, abilityMin, abilityMax = setupMultivariateModel(pctbcvchick_dataset, 'Dreamer_basic_control_Success', 'Dreamer_basic_control_Choice')\n",
        "with model_dreamer_basic_control:\n",
        "  data_dreamer_basic_control = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "v-hdjgMuNxEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "capability_forest_plot(data_random['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_heuristic['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_ppo_basic['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_ppo_basic_control['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_dreamer_basic['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])\n",
        "capability_forest_plot(data_dreamer_basic_control['posterior'][['objPermAbility']], ax_min = abilityMin['objPermAbility'], ax_max = abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "h2Gnr6sPPddM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to continue playing around with these models in the remainder of the session."
      ],
      "metadata": {
        "id": "aBxpNXRFTQMZ"
      }
    }
  ]
}