{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNV70/KwwyrNwQ41cFXtwku",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/blob/main/tutorial-notebooks/4_BuildingGoodBenchmarks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building Good Benchmarks\n",
        "\n",
        "**Lead Presenter**: Kozzy Voudouris\n",
        "\n",
        "In this tutorial, we introduce some key concepts that should guide the development of purpose-built benchmarks for use with measurement layouts. In this notebook, we incrementally build a measurement layout for studying the cognitive capability of object permanence in a complex three-dimensional environment. Finally, We evaluate some agents on a suite of tests."
      ],
      "metadata": {
        "id": "EOQkgyhAQMZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preamble\n",
        "\n",
        "First, let's import the libraries, functions, and data that we will need."
      ],
      "metadata": {
        "id": "uuPrpVEizOiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arviz --quiet\n",
        "!pip install erroranalysis --quiet\n",
        "!pip install numpy --quiet\n",
        "!pip install pymc --quiet"
      ],
      "metadata": {
        "id": "e-_FrQEczOLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import arviz as az\n",
        "import erroranalysis as ea\n",
        "import gc\n",
        "import graphviz\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pymc as pm\n",
        "import random as rm\n",
        "import seaborn as sns\n",
        "\n",
        "from IPython.display import Image\n",
        "from scipy import stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import files\n",
        "from pymc import model\n",
        "\n",
        "print(f\"Running on PyMC v{pm.__version__}\") #Note, colab imports an older version of PyMC by default. This won't cause problems for this tutorial, but may do if you use a different backend (e.g., gpu) and a jax/numpyro sampler. In which case, run `!pip install 'pymc>5.9' --quiet`"
      ],
      "metadata": {
        "id": "nNjpBg-Pzu5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/Kinds-of-Intelligence-CFI/measurement-layout-tutorial/main/data/4_PCTB_data.csv'\n",
        "pctb_dataset = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "e-D77_5w0O-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect the dataset. There are **551 instances**, **8 metafeatures**, and the performances of **7 agents**.\n",
        "\n",
        "The metafeatures are as follows:\n",
        "1. `basicTask` - is the task a basic task? Values (discrete, binary): `0` (No), `1` (Yes).\n",
        "2. `pctbGridTask` - is the task a PCTB Grid task? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "3. `mainGoalSize` - what size is the goal in the task? Value range: `0.5` - `5.0`.\n",
        "4. `goalPosition` - what is the relative position of the goal with respect to the agent's starting position? Value range: `-1.5` - `1.5` (0 is centre point).\n",
        "5. `goalOccluded` - is the goal occluded when the agent starts the episode? Values (discrete binary): `0` (No), `1` (Yes).\n",
        "6. `minDistToGoal` - how far is the goal from the agent? This is calculated is the manhattan distance to the goal, avoiding any obstacles/pits. Value range: `9.0` - `47.0`.\n",
        "7. `minNumTurnsGoal` - how many right-angle turns would the agent take on the trajectory described by `minDistToGoal`. Value range: `0.0` - `3.0`.\n",
        "8. `numChoices` - how many choices does the agent have in the task? Value range: `1.0` - `12.0`.\n",
        "\n",
        "The agents performed each of these tasks, and whether they obtained the goal (`1`) or not (`0`) was recorded. The agents are as follows:\n",
        "1. `Random_Agent` - An agent which randomly samples one of the 9 actions in the Animal-AI Environment (`no action`, `forwards`, `backwards`, `left rotate`, `right rotate`, `forwards left`, `forwards right`, `backwards left`, `backwards right`). It then takes that action for a number of steps sampled from $U(1, 20)$.\n",
        "2. `Heuristic_Agent` - An agent that navigates towards green goals, following a rigid rule.\n",
        "3. `Dreamer_basic` - A dreamer-v3 agent trained for 2M steps on a set of 300 basic tasks, of which all tasks where `basicTask == 1` are a subset.\n",
        "4. `Dreamer_basic_control` - A dreamer-v3 agent trained for 10M steps on a set of 2372 basic and practice tasks, of which all tasks where `basicTask == 1 or (pctbGridTask == 1 and goalOccluded == 0)` are a subset.\n",
        "5. `PPO_ basic` - A PPO agent trained for 2M steps on a set of 300 basic tasks, of which all tasks where `basicTask == 1` are a subset.\n",
        "6. `PPO_basic_control` - A PPO agent trained for 10M steps on a set of 2372 basic and practice tasks, of which all tasks where `basicTask == 1 or (pctbGridTask == 1 and goalOccluded == 0)` are a subset."
      ],
      "metadata": {
        "id": "EH9a5LS60luD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pctb_dataset"
      ],
      "metadata": {
        "id": "sbsoI-vN0j38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Object Permanence\n",
        "\n",
        "First, let's set up a simple measurement layout for the *Object Permanence* ability.\n",
        "\n",
        "The leaf node of the measurement layout is success in this case, so we want to define it as a Bernoulli, taking a probability of success. Therefore, we'll need a logistic function. Because we are dealing with bounded capabilities, the logistic function means that the probability of success on a task with a minimum demand for an agent with maximum ability is 0.999. Alternatively, for an agent with minimum ability performing on a task with maximum demand, we get a probability of success of 0.001. This is a nice parameterisation of the logistic function for our case of bounded capabilities."
      ],
      "metadata": {
        "id": "jR5RHB45zKpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic999(x, min, max):    # This logistic function ensures that if x is at -(max-min), we get prob 0.001, and if x is at (max-min), we get prob 0.999\n",
        "  x = x - min\n",
        "  max = max - min\n",
        "  x = 6.90675478 * x / max\n",
        "  return 1 / (1 + np.exp(-x))"
      ],
      "metadata": {
        "id": "3eNXs1trEy4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTWHzmLlQHfK"
      },
      "outputs": [],
      "source": [
        "def setupOPModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", objPermP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPModel(pctb_dataset, 'Dreamer_basic')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "rzjj9VM0GIaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this measurement layout on two of our agents: the `Dreamer_basic` agent and the `Heuristic_Agent`."
      ],
      "metadata": {
        "id": "PJtPtgtqI970"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dreamer_basic, abilityMin, abilityMax = setupOPModel(pctb_dataset, 'Dreamer_basic')\n",
        "with model_dreamer_basic:\n",
        "  data_dreamer_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPModel(pctb_dataset, 'Heuristic_Agent')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "8s0nYG1FGvDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the inferred object permanence capability of these agents, by plotting their capabilities as forest plots:"
      ],
      "metadata": {
        "id": "edXKGnXHJPDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_dreamer_basic = az.plot_forest(data=data_dreamer_basic['posterior'][['objPermAbility']])\n",
        "axes_dreamer_basic = forest_plot_dreamer_basic.ravel()[0]\n",
        "axes_dreamer_basic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "52tC1XRwHNKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at the summary statistics from the two measurement layouts:"
      ],
      "metadata": {
        "id": "-pn-N5ZzJ1Gb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_dreamer_basic = az.summary(data_dreamer_basic['posterior']['objPermAbility'])\n",
        "summary_dreamer_basic"
      ],
      "metadata": {
        "id": "PXUlQ2h6Hd37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_heuristic = az.summary(data_heuristic['posterior']['objPermAbility'])\n",
        "summary_heuristic"
      ],
      "metadata": {
        "id": "a9DVFTaKJ-YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are inferring that the heuristic agent has higher object permanence than the dreamer agent."
      ],
      "metadata": {
        "id": "9Qy1wpYQKDcy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introducing Navigation\n",
        "\n",
        "These tasks are fundamentally search tasks. The agent must navigate towards the reward. As such, an agent with object permanence but poor at navigation may fail many of these tasks. Moreover, an agent without object permanence, but that is good at navigating, may accidentally obtain the reward on occasion.\n",
        "\n",
        "The simplest way to frame this is to say that navigation and object permanence are non-compensatory - being good at navigation does not compensate for being bad at object permanence, and vice versa. For the purposes of this tutorial, we can proceed with this formulation, although it may be more accurate to implement an asymmetric compensatory relationship between these too (since, arguably, navigation is more compensatory for object permanence than vice versa).\n",
        "\n",
        "Navigation demands can be implemented in terms of how far away the goal is along with the circuitousness of the route. We can define this as the product of distance and number of turns.\n",
        "\n",
        "Let's extend the measurement layout to include navigation:"
      ],
      "metadata": {
        "id": "ILpMtAGVKVS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setupOPNavModel(data, agent_col_name: str):\n",
        "\n",
        "  # get results column\n",
        "  results = data[agent_col_name]\n",
        "\n",
        "  # define bounds\n",
        "  abilityMin = {}\n",
        "  abilityMax = {}\n",
        "\n",
        "  minPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).min())\n",
        "  minNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).min())\n",
        "\n",
        "  maxPermAbility = ((data[\"minDistToGoal\"] * data[\"numChoices\"]).max())\n",
        "  maxNavAbility = ((data[\"minDistToGoal\"] * data[\"minNumTurnsGoal\"]).max())\n",
        "\n",
        "  abilityMin[\"objPermAbility\"] = minPermAbility\n",
        "  abilityMax[\"objPermAbility\"] = maxPermAbility\n",
        "\n",
        "  abilityMin[\"navAbility\"] = minNavAbility\n",
        "  abilityMax[\"navAbility\"] = maxNavAbility\n",
        "\n",
        "\n",
        "  m = pm.Model()\n",
        "  with m:\n",
        "\n",
        "    # Define abilities and their priors\n",
        "\n",
        "    objPermAbility = pm.Uniform(\"objPermAbility\", minPermAbility, maxPermAbility)\n",
        "\n",
        "    navAbility = pm.Uniform(\"navAbility\", minNavAbility, maxNavAbility)\n",
        "\n",
        "    # Define environment variables as MutableData\n",
        "\n",
        "    goalDist = pm.MutableData(\"goalDistance\", data[\"minDistToGoal\"].values)\n",
        "    numChoices = pm.MutableData(\"numChoices\", data[\"numChoices\"].values)\n",
        "    opTest = pm.MutableData(\"goalOccluded\", data[\"goalOccluded\"].values)\n",
        "    numTurnsGoal = pm.MutableData(\"minTurnsToGoal\", data[\"minNumTurnsGoal\"].values)\n",
        "\n",
        "    # Margins\n",
        "\n",
        "    objPermMargin = (objPermAbility - (goalDist * numChoices * opTest))\n",
        "    objPermP = pm.Deterministic(\"objPermP\", logistic999(objPermMargin, min = minPermAbility, max = maxPermAbility))\n",
        "\n",
        "    navP = pm.Deterministic(\"navP\", logistic999(navAbility - (goalDist * numTurnsGoal), min = minNavAbility, max = maxNavAbility))\n",
        "\n",
        "    # Define final margin with non-compensatory interaction\n",
        "\n",
        "    finalP = pm.Deterministic(\"finalP\", (objPermP * navP))\n",
        "\n",
        "    taskSuccess = pm.Bernoulli(\"taskSuccess\", finalP, observed = results)\n",
        "\n",
        "  return m, abilityMin, abilityMax"
      ],
      "metadata": {
        "id": "Eai-I-daRPXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "m, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Dreamer_basic')\n",
        "gv = pm.model_graph.model_to_graphviz(m)\n",
        "gv"
      ],
      "metadata": {
        "id": "BEcPZlisSVdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run this new measurement layout with the same two agents:"
      ],
      "metadata": {
        "id": "z6D4NpKBSpJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_dreamer_basic, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Dreamer_basic')\n",
        "with model_dreamer_basic:\n",
        "  data_dreamer_basic = pm.sample(1000, target_accept=0.95)\n",
        "\n",
        "model_heuristic, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Heuristic_Agent')\n",
        "with model_heuristic:\n",
        "  data_heuristic = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "h3JUouDISozT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the inferred object permanence and navigation capabilities of these agents, by plotting their capabilities as forest plots:"
      ],
      "metadata": {
        "id": "r0XTu2k3S5EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_dreamer_basic = az.plot_forest(data=data_dreamer_basic['posterior'][['objPermAbility']])\n",
        "axes_dreamer_basic = forest_plot_dreamer_basic.ravel()[0]\n",
        "axes_dreamer_basic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['objPermAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "iEnF2gJqSw2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_dreamer_basic = az.plot_forest(data=data_dreamer_basic['posterior'][['navAbility']])\n",
        "axes_dreamer_basic = forest_plot_dreamer_basic.ravel()[0]\n",
        "axes_dreamer_basic.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])\n",
        "\n",
        "forest_plot_heuristic = az.plot_forest(data=data_heuristic['posterior'][['navAbility']])\n",
        "axes_heuristic = forest_plot_heuristic.ravel()[0]\n",
        "axes_heuristic.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "U0dY9fdMTEBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare these results to the random agent too:"
      ],
      "metadata": {
        "id": "v82UIAyXTjRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_random, abilityMin, abilityMax = setupOPNavModel(pctb_dataset, 'Random_Agent')\n",
        "with model_random:\n",
        "  data_random = pm.sample(1000, target_accept=0.95)"
      ],
      "metadata": {
        "id": "XcqnOJ0GTvCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['objPermAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['objPermAbility'], right=abilityMax['objPermAbility'])"
      ],
      "metadata": {
        "id": "5mtUIPFPT1Dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forest_plot_random = az.plot_forest(data=data_random['posterior'][['navAbility']])\n",
        "axes_random = forest_plot_random.ravel()[0]\n",
        "axes_random.set_xlim(left=abilityMin['navAbility'], right=abilityMax['navAbility'])"
      ],
      "metadata": {
        "id": "f_0a8CLaT1iL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}